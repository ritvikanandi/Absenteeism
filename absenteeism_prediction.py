# -*- coding: utf-8 -*-
"""Absenteeism prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uVOC53QuGARMwRXpcJnqOerTgr2fdV16

**The following analysis has been done to predict the absenteeism at a workplace during the office hours.**

Attribute Information:

1. Individual identification (ID)
2. Reason for absence (ICD).
Absences attested by the International Code of Diseases (ICD) stratified into 21 categories (I to XXI) as follows:

I Certain infectious and parasitic diseases

II Neoplasms

III Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism

IV Endocrine, nutritional and metabolic diseases

V Mental and behavioural disorders

VI Diseases of the nervous system

VII Diseases of the eye and adnexa

VIII Diseases of the ear and mastoid process

IX Diseases of the circulatory system

X Diseases of the respiratory system

XI Diseases of the digestive system

XII Diseases of the skin and subcutaneous tissue

XIII Diseases of the musculoskeletal system and connective tissue

XIV Diseases of the genitourinary system

XV Pregnancy, childbirth and the puerperium

XVI Certain conditions originating in the perinatal period

XVII Congenital malformations, deformations and chromosomal abnormalities

XVIII Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified

XIX Injury, poisoning and certain other consequences of external causes

XX External causes of morbidity and mortality

XXI Factors influencing health status and contact with health services.

And 7 categories without (CID) patient follow-up (22), medical consultation (23), blood donation (24), laboratory examination (25), unjustified absence (26), physiotherapy (27), dental consultation (28).
3. Month of absence
4. Day of the week (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))
5. Seasons (summer (1), autumn (2), winter (3), spring (4))
6. Transportation expense
7. Distance from Residence to Work (kilometers)
8. Service time
9. Age
10. Work load Average/day
11. Hit target
12. Disciplinary failure (yes=1; no=0)
13. Education (high school (1), graduate (2), postgraduate (3), master and doctor (4))
14. Son (number of children)
15. Social drinker (yes=1; no=0)
16. Social smoker (yes=1; no=0)
17. Pet (number of pet)
18. Weight
19. Height
20. Body mass index
21. Absenteeism time in hours (target)

<h1>Importing Libraries</h1>
"""

import pandas as pd
import numpy as np

"""<h1>Reading Data</h1>"""

data = pd.read_csv('Absenteeism_at_work.csv', delimiter=';')

data.head()

data.describe()

type(data)

#checking for null values
data.info()

data.isnull().sum()

"""<h1>Data Preprocessing</h1>"""

#Drop ID column
data = data.drop(['ID'], axis=1)

data

#grouping reasons for absence
sorted(data['Reason for absence'].unique())

data['Reason for absence'].value_counts()

data.columns.values

reasons_dummies = pd.get_dummies(data['Reason for absence'])
reasons_dummies = pd.get_dummies(data['Reason for absence'], drop_first=True)
reasons_dummies

reasons_dummies.columns.values

data = data.drop(['Reason for absence'], axis=1)
data.head()

reason_type_1 = reasons_dummies.loc[:, 1:14].max(axis=1)
reason_type_2 = reasons_dummies.loc[:, 15:17].max(axis=1)
reason_type_3 = reasons_dummies.loc[:, 18:21].max(axis=1)
reason_type_4 = reasons_dummies.loc[:, 22:].max(axis=1)

#concatenation
data = pd.concat([data, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis=1)

data.head()

data.columns.values

data_columns = ['Month of absence', 'Day of the week', 'Seasons',
       'Transportation expense', 'Distance from Residence to Work',
       'Service time', 'Age', 'Work load Average/day ', 'Hit target',
       'Disciplinary failure', 'Education', 'Son', 'Social drinker',
       'Social smoker', 'Pet', 'Weight', 'Height', 'Body mass index',
       'Absenteeism time in hours', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4']

data.columns = data_columns
data.head()

#reordering columns
column_names = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month of absence', 'Day of the week', 'Seasons',
       'Transportation expense', 'Distance from Residence to Work',
       'Service time', 'Age', 'Work load Average/day ', 'Hit target',
       'Disciplinary failure', 'Education', 'Son', 'Social drinker',
       'Social smoker', 'Pet', 'Weight', 'Height', 'Body mass index',
       'Absenteeism time in hours']

data = data[column_names]
data.head()

#month of absence
data['Month of absence'].unique()
data['Month of absence'].value_counts()
#copy
df = data.loc[data['Month of absence'] != 0]
df.shape

df.tail()

df['Day of the week'].unique()

df["Seasons"].unique()

df['Seasons'].value_counts()

df['Hit target'].unique()

df['Hit target'].value_counts()

df['Education'].value_counts()

#classifying education to only 2 groups
df['Education'] = df['Education'].map({1:0, 2:1, 3:1, 4:1})

df['Education'].value_counts()

df.head()

df['Son'].unique()

df['Son'].value_counts()

df_preprocessed = df.copy()

df_preprocessed.to_csv('Absenteeism_preprocessed.csv', index=False)

"""<h1>Machine Learning</h1>"""

#creating the targets
df_preprocessed['Absenteeism time in hours'].median()

#dividing into two classes
targets = np.where(df_preprocessed['Absenteeism time in hours'] > 3, 1, 0)
targets

df_preprocessed['Excessive Absenteeism'] = targets
df_preprocessed.head()

targets.sum()/targets.shape[0]

df_preprocessed = df_preprocessed.drop(['Absenteeism time in hours'], axis=1)
df_preprocessed.head()

"""**Input Selection**"""

unscaled_inputs = df_preprocessed.iloc[:, :-1]

unscaled_inputs.head()

unscaled_inputs.shape

"""**Data Scaling**"""

#Standardize the data
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler

# create the Custom Scaler class

class CustomScaler(BaseEstimator,TransformerMixin): 
    
    # init or what information we need to declare a CustomScaler object
    # and what is calculated/declared as we do
    
    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        
        # scaler is nothing but a Standard Scaler object
        self.scaler = StandardScaler(copy,with_mean,with_std)
        # with some columns 'twist'
        self.columns = columns
        self.mean_ = None
        self.var_ = None
        
    
    # the fit method, which, again based on StandardScale
    
    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.mean(X[self.columns])
        self.var_ = np.var(X[self.columns])
        return self
    
    # the transform method which does the actual scaling

    def transform(self, X, y=None, copy=None):
        
        # record the initial order of the columns
        init_col_order = X.columns
        
        # scale all features that you chose when creating the instance of the class
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        
        # declare a variable containing all information that was not scaled
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        
        # return a data frame which contains all scaled features and all 'not scaled' features
        # use the original order (that you recorded in the beginning)
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]

columns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']

columns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]

absenteeism_scaler = CustomScaler(columns_to_scale)
absenteeism_scaler.fit(unscaled_inputs)

scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)

scaled_inputs

scaled_inputs.shape

"""**Data Splitting**"""

from sklearn.model_selection import train_test_split

xtrain, xtest, ytrain, ytest = train_test_split(scaled_inputs, targets, test_size=0.20, random_state=20)

xtrain.shape

xtest.shape

ytest.shape

ytrain.shape

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

"""**Training**"""

reg = LogisticRegression()

reg.fit(xtrain, ytrain)

reg.score(xtrain, ytrain)

ypredict = reg.predict(xtest)

from sklearn.metrics import accuracy_score

log_reg = accuracy_score(ytest, ypredict)
log_reg

"""**Finding Coefficients and intercept**"""

reg.intercept_

reg.coef_

feature_name = unscaled_inputs.columns.values

summary_table = pd.DataFrame(columns=['Feature_name'], data=feature_name)
summary_table['Coefficient'] = np.transpose(reg.coef_)
summary_table

summary_table.index = summary_table.index+1
summary_table.loc[0] = ['Intercept', reg.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table

summary_table['odds_ratio'] = np.exp(summary_table.Coefficient)
summary_table

summary_table.sort_values('odds_ratio', ascending=False)

"""**Save the model**"""

import pickle

with open('model', 'wb') as file:
  pickle.dump(reg, file)

with open('scaler', 'wb') as file:
  pickle.dump(absenteeism_scaler, file)